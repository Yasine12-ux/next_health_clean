{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aziz\n",
      "[nltk_data]     Hlila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aziz\n",
      "[nltk_data]     Hlila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLTK's SnowballStemmer and stopwords for French\n",
    "stemmer = SnowballStemmer('french')\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "\n",
    "def tokenizeText(text, dictionary):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text using NLTK tokenizer\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming using SnowballStemmer\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Numerize tokens\n",
    "    numerical_tokens = []\n",
    "    for token in stemmed_tokens:\n",
    "        if token not in dictionary:\n",
    "            # dictionary[token] = torch.tensor(len(dictionary) + 1,dtype=torch.int32)\n",
    "            dictionary[token] = len(dictionary) + 1\n",
    "        numerical_tokens.append(dictionary[token])\n",
    "    \n",
    "    # Convert numerical tokens and dictionary values to tensors\n",
    "    # numerical_tokens_tensor = torch.tensor(numerical_tokens)\n",
    "    numerical_tokens_np = np.array(numerical_tokens)\n",
    "    # return numerical_tokens_tensor\n",
    "    return numerical_tokens_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"data_cleanOutput.csv\")\n",
    "# data = data.drop_duplicates()\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with sklean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "1      [1, 2, 10, 11, 12, 13, 14, 8, 9]\n",
       "2      [1, 2, 10, 13, 15, 16, 17, 8, 9]\n",
       "3              [1, 2, 18, 19, 20, 8, 9]\n",
       "4      [1, 2, 10, 21, 22, 23, 24, 8, 9]\n",
       "                     ...               \n",
       "315       [1, 2, 27, 13, 108, 49, 8, 9]\n",
       "316             [1, 2, 18, 7, 19, 8, 9]\n",
       "317       [1, 2, 27, 109, 94, 74, 8, 9]\n",
       "318             [1, 2, 18, 44, 6, 8, 9]\n",
       "319           [1, 2, 27, 101, 49, 8, 9]\n",
       "Name: Description, Length: 320, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary={}\n",
    "descriptionsTokenized = df_full['Description'].apply(lambda x: tokenizeText(x, dictionary))\n",
    "descriptionsTokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 300)\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sequence_list, maxlen=None):\n",
    "    if not maxlen:\n",
    "        maxlen = max(len(seq) for seq in sequence_list)\n",
    "    padded_sequences = np.zeros((len(sequence_list), maxlen), dtype=int)\n",
    "    for i, seq in enumerate(sequence_list):\n",
    "        if len(seq) > 0:  # Check if the sequence is not empty\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "    return padded_sequences\n",
    "\n",
    "descriptionsTokenizedPadded = pad_sequences(descriptionsTokenized,300)\n",
    "print(descriptionsTokenizedPadded.shape)\n",
    "df_tokens = pd.DataFrame(data=descriptionsTokenizedPadded,index=df_full.index).rename(columns=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_full.drop(columns=['Ordonnance'])\n",
    "y = df_full['Ordonnance']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before  (320, 22) after  (320, 30)\n",
      "(320, 330)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of numeric and categorical features\n",
    "numeric_features = ['tailleCm', 'poidsKg', 'IMC', 'nbGrossesse', 'nbEnfantsVivants',\n",
    "                    'nbMacrosomies', 'nbAvortements', 'nbMortNes', 'ageMenopause',\n",
    "                    'alcoolSemaine', 'nbCigaretteParJour', 'Age']\n",
    "categorical_features = ['groupeSanguin', 'HTA', 'diabete', 'dyslipidemie',\n",
    "                        'tabacStatus',\n",
    "                        'drogue',]\n",
    "\n",
    "\n",
    "# Preprocessing pipelines for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing to numeric and categorical features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "print(\"before \",X.shape,\"after \",X_preprocessed.shape)\n",
    "X_combined = np.hstack((X_preprocessed, descriptionsTokenizedPadded))\n",
    "print(X_combined.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessor.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save preproccesser\n",
    "from joblib import dump, load\n",
    "pippreprocessor_file = 'preprocessor.joblib'\n",
    "dump(preprocessor, pippreprocessor_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.44038222,  0.327272  , -0.71714759, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.56828561, -1.04350077, -1.18375748, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.77082628, -0.12965226, -0.80198575, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.63691509,  0.96696596,  0.97961565, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.23784155, -1.59180988, -1.56552921, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.84211579,  1.69804477,  0.89477749, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "pipeline_file = 'pipeline.joblib'\n",
    "\n",
    "def concatenate_features(X):\n",
    "    return np.hstack((X, descriptionsTokenizedPadded))\n",
    "\n",
    "preprocessor=load(pippreprocessor_file)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_concatenation', FunctionTransformer(concatenate_features, validate=False))\n",
    "])\n",
    "\n",
    "# Fit and transform your data\n",
    "X_combined = pipeline.fit_transform(X)\n",
    "\n",
    "# Later, you can load the pipeline from the file\n",
    "loaded_pipeline = load(pipeline_file)\n",
    "loaded_pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tailleCm</th>\n",
       "      <th>poidsKg</th>\n",
       "      <th>groupeSanguin</th>\n",
       "      <th>IMC</th>\n",
       "      <th>adresse</th>\n",
       "      <th>HTA</th>\n",
       "      <th>diabete</th>\n",
       "      <th>dyslipidemie</th>\n",
       "      <th>autresAntecedentsFamiliaux</th>\n",
       "      <th>nbGrossesse</th>\n",
       "      <th>...</th>\n",
       "      <th>ageMenopause</th>\n",
       "      <th>autresAntecedentsGynecoObstetriques</th>\n",
       "      <th>alcoolSemaine</th>\n",
       "      <th>tabacStatus</th>\n",
       "      <th>nbCigaretteParJour</th>\n",
       "      <th>drogue</th>\n",
       "      <th>autreHabitudeToxique</th>\n",
       "      <th>Age</th>\n",
       "      <th>sexe</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>75</td>\n",
       "      <td>O+</td>\n",
       "      <td>23.1</td>\n",
       "      <td>123 Rue</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>non-fumeur</td>\n",
       "      <td>0.0</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "      <td>42</td>\n",
       "      <td>homme</td>\n",
       "      <td>\"Le patient souffre de maux de tête fréquents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tailleCm  poidsKg groupeSanguin   IMC  adresse  HTA diabete dyslipidemie  \\\n",
       "0       180       75            O+  23.1  123 Rue  non     non          oui   \n",
       "\n",
       "   autresAntecedentsFamiliaux  nbGrossesse  ...  ageMenopause  \\\n",
       "0                         NaN          0.0  ...           0.0   \n",
       "\n",
       "   autresAntecedentsGynecoObstetriques  alcoolSemaine  tabacStatus  \\\n",
       "0                                  NaN              0   non-fumeur   \n",
       "\n",
       "   nbCigaretteParJour  drogue  autreHabitudeToxique  Age   sexe  \\\n",
       "0                 0.0     non                   non   42  homme   \n",
       "\n",
       "                                         Description  \n",
       "0   \"Le patient souffre de maux de tête fréquents...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv(\"validation.csv\")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 320",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\STUDY\\Genie Logiciel ISIMM\\DevPFE\\backend\\healthcare-pfe\\MachineLearningDev\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:906\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    904\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[1;32m--> 906\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params[name]\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32md:\\STUDY\\Genie Logiciel ISIMM\\DevPFE\\backend\\healthcare-pfe\\MachineLearningDev\\.venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32md:\\STUDY\\Genie Logiciel ISIMM\\DevPFE\\backend\\healthcare-pfe\\MachineLearningDev\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:252\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 252\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m output_config \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# check the consistency between the column provided by `transform` and\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# the the column names provided by `get_feature_names_out`.\u001b[39;00m\n",
      "File \u001b[1;32md:\\STUDY\\Genie Logiciel ISIMM\\DevPFE\\backend\\healthcare-pfe\\MachineLearningDev\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:379\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kw_args \u001b[38;5;28;01mif\u001b[39;00m kw_args \u001b[38;5;28;01melse\u001b[39;00m {}))\n",
      "Cell \u001b[1;32mIn[52], line 4\u001b[0m, in \u001b[0;36mconcatenate_features\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate_features\u001b[39m(X):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptionsTokenizedPadded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\STUDY\\Genie Logiciel ISIMM\\DevPFE\\backend\\healthcare-pfe\\MachineLearningDev\\.venv\\lib\\site-packages\\numpy\\core\\shape_base.py:359\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 320"
     ]
    }
   ],
   "source": [
    "pipeline.transform(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40625\n",
      "(64, 352)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(X_test, n_neighbors=10)\n",
    "# Convert indices to actual predictions\n",
    "top_k_predictions = np.array(y_train)[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Accuracy: 0.89\n",
      "Mean Reciprocal Rank: 0.87\n",
      "Coverage at 3: 0.85\n"
     ]
    }
   ],
   "source": [
    "def top_n_accuracy(y_true, y_pred, n=3):\n",
    "    top_n_correct = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true in pred[:n]:\n",
    "            top_n_correct += 1\n",
    "    return top_n_correct / len(y_true)\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_pred):\n",
    "    rr_sum = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true in pred:\n",
    "            rr_sum += 1 / (pred.tolist().index(true) + 1)\n",
    "    return rr_sum / len(y_true)\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=3):\n",
    "    precision_sum = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true in pred[:k]:\n",
    "            precision_sum += 1\n",
    "    return precision_sum / (k * len(y_true))\n",
    "\n",
    "def coverage(y_pred, all_items, k=3):\n",
    "    recommended_items = set()\n",
    "    for pred in y_pred:\n",
    "        recommended_items.update(pred[:k])\n",
    "    return len(recommended_items) / len(all_items)\n",
    "\n",
    "# Example: Evaluating the top 3 recommendations\n",
    "top_3_acc = top_n_accuracy(y_test, top_k_predictions, n=7)\n",
    "mrr = mean_reciprocal_rank(y_test, top_k_predictions)\n",
    "precision_k = precision_at_k(y_test, top_k_predictions, k=7)\n",
    "all_possible_items = set(y_train)  # Use the unique items from the training set as all possible items\n",
    "coverage_score = coverage(top_k_predictions, all_possible_items, k=7)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Top-3 Accuracy: {top_3_acc:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {mrr:.2f}\")\n",
    "print(f\"Coverage at 3: {coverage_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export model with dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)\n",
    "pickle.dump(knn, 'knn_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = pickle.load(\"knn_model.joblib\")\n",
    "with open('tokenizer_dict.pkl', 'rb') as f:\n",
    "    tokenizer_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' \"Prendre un comprimé de lisinopril 10mg une fois par jour.\"',\n",
       "       ' \"Prendre metformine 500mg deux fois par jour et amlodipine 5mg une fois par jour.\"',\n",
       "       ' \"Prescrire un antispasmodique à prendre au besoin.\"',\n",
       "       ' \"Prescrire un bêta-bloquant et une statine à prendre quotidiennement.\"',\n",
       "       ' \"Prendre fer 80mg une fois par jour.\"',\n",
       "       ' \"Prescrire un antitussif et recommander une radiographie thoracique.\"',\n",
       "       ' \"Prescrire un bêta-bloquant et une statine à prendre quotidiennement.\"',\n",
       "       ' \"Prescrire un triptan à prendre au début des symptômes de la migraine.\"',\n",
       "       ' \"Prescrire un analgésique et recommander des séances de kinésithérapie.\"',\n",
       "       ' \"Prescrire un triptan à prendre au début des symptômes de la migraine.\"',\n",
       "       ' \"Prescrire un diurétique et recommander un régime pauvre en sel.\"',\n",
       "       ' \"Administrer 10 unités d\\'insuline avant chaque repas.\"',\n",
       "       ' \"Prescrire un antitussif et recommander une radiographie thoracique.\"',\n",
       "       ' \"Prendre metformine 500mg deux fois par jour et lisinopril 10mg une fois par jour.\"',\n",
       "       ' \"Appliquer une pommade anti-inflammatoire deux fois par jour.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre un comprimé de fer 100mg par jour.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Administrer 10 unités d\\'insuline avant chaque repas.\"',\n",
       "       ' \"Prendre atorvastatine 20mg une fois par jour.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prescrire des anti-inflammatoires non stéroïdiens et recommander des séances de physiothérapie.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre paracétamol 500mg au besoin pour la douleur.\"',\n",
       "       ' \"Prescrire un antihistaminique à prendre quotidiennement.\"',\n",
       "       ' \"Appliquer une pommade anti-inflammatoire deux fois par jour.\"',\n",
       "       ' \"Prendre paracétamol 500mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre un comprimé de dompéridone 10mg trois fois par jour.\"',\n",
       "       ' \"Prescrire un analgésique et recommander des séances de kinésithérapie.\"',\n",
       "       ' \"Prendre un comprimé d\\'aspirine 100mg une fois par jour.\"',\n",
       "       ' \"Prendre atorvastatine 20mg une fois par jour.\"',\n",
       "       ' \"Prendre un comprimé de dompéridone 10mg trois fois par jour.\"',\n",
       "       ' \"Prescrire un triptan à prendre au début des symptômes de la migraine.\"',\n",
       "       ' \"Administrer 20 unités d\\'insuline chaque soir.\"',\n",
       "       ' \"Prendre sumatriptan 50mg au besoin.\"',\n",
       "       ' \"Prendre 600mg d\\'ibuprofène au besoin pour la douleur.\"',\n",
       "       ' \"Administrer 10 unités d\\'insuline avant chaque repas.\"',\n",
       "       ' \"Administrer 10 unités d\\'insuline avant chaque repas.\"',\n",
       "       ' \"Prescrire un analgésique et recommander des séances de kinésithérapie.\"',\n",
       "       ' \"Prescrire un diurétique et recommander un régime pauvre en sel.\"',\n",
       "       ' \"Prescrire un antiangineux et recommander une réduction de la consommation de sel.\"',\n",
       "       ' \"Prendre des anti-inflammatoires non stéroïdiens pendant 5 jours.\"',\n",
       "       ' \"Prescrire un antispasmodique à prendre au besoin.\"',\n",
       "       ' \"Prendre paracétamol 500mg au besoin pour la douleur.\"',\n",
       "       ' \"Appliquer une pommade anti-inflammatoire deux fois par jour.\"',\n",
       "       ' \"Prescrire un diurétique et recommander un régime pauvre en sel.\"',\n",
       "       ' \"Prendre un comprimé de dompéridone 10mg trois fois par jour.\"',\n",
       "       ' \"Prendre fer 80mg une fois par jour.\"',\n",
       "       ' \"Prescrire un diurétique et recommander un régime pauvre en sel.\"',\n",
       "       ' \"Prendre paracétamol 500mg au besoin pour la douleur.\"',\n",
       "       ' \"Prendre sumatriptan 50mg au besoin pour les migraines.\"',\n",
       "       ' \"Administrer 10 unités d\\'insuline avant chaque repas.\"',\n",
       "       ' \"Prendre des anti-inflammatoires non stéroïdiens pendant 5 jours.\"',\n",
       "       ' \"Prendre metformine 500mg deux fois par jour et lisinopril 10mg une fois par jour.\"',\n",
       "       ' \"Prescrire un inhibiteur de la pompe à protons à prendre une fois par jour avant le petit-déjeuner.\"',\n",
       "       ' \"Prendre 1 comprimé de 500 mg de metformine deux fois par jour et un comprimé de 10 mg de lisinopril chaque matin.\"',\n",
       "       ' \"Prendre ibuprofène 400mg au besoin pour la douleur.\"',\n",
       "       ' \"Prescrire des anti-inflammatoires non stéroïdiens et recommander des séances de physiothérapie.\"',\n",
       "       ' \"Prendre metformine 500mg deux fois par jour et amlodipine 5mg une fois par jour.\"',\n",
       "       ' \"Prescrire un antitussif et recommander une radiographie thoracique.\"',\n",
       "       ' \"Prendre un comprimé de dompéridone 10mg trois fois par jour.\"',\n",
       "       ' \"Prendre diclofénac 50mg au besoin pour la douleur.\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Decision Tree model on the training data\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the leaf node for the test data\n",
    "leaf_nodes = decision_tree.apply(X_test)\n",
    "\n",
    "# Get the samples that fall into each leaf node in the training data\n",
    "train_leaf_nodes = decision_tree.apply(X_train)\n",
    "leaf_node_to_samples = {leaf_node: [] for leaf_node in np.unique(train_leaf_nodes)}\n",
    "for idx, leaf_node in enumerate(train_leaf_nodes):\n",
    "    leaf_node_to_samples[leaf_node].append(idx)\n",
    "\n",
    "# Get top 3 predictions based on majority class in each leaf node\n",
    "top_k_predictions = []\n",
    "for leaf_node in leaf_nodes:\n",
    "    samples = leaf_node_to_samples[leaf_node]\n",
    "    top_labels = [y_train.iloc[i] for i in samples]\n",
    "    top_3_labels = [label for label, _ in Counter(top_labels).most_common(3)]\n",
    "    top_k_predictions.append(top_3_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_k_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recommended_items) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_possible_items)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate the recommendations\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m top_3_acc \u001b[38;5;241m=\u001b[39m top_n_accuracy(y_test, \u001b[43mtop_k_predictions\u001b[49m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     27\u001b[0m mrr \u001b[38;5;241m=\u001b[39m mean_reciprocal_rank(y_test, top_k_predictions)\n\u001b[0;32m     28\u001b[0m all_possible_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(y_train)  \u001b[38;5;66;03m# Use the unique items from the training set as all possible items\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_k_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Define evaluation functions\n",
    "def top_n_accuracy(y_true, top_n_preds, n=3):\n",
    "    correct = 0\n",
    "    for true, preds in zip(y_true, top_n_preds):\n",
    "        if true in preds[:n]:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def mean_reciprocal_rank(y_true, top_n_preds):\n",
    "    ranks = []\n",
    "    for true, preds in zip(y_true, top_n_preds):\n",
    "        if true in preds:\n",
    "            rank = preds.index(true) + 1\n",
    "            ranks.append(1 / rank)\n",
    "        else:\n",
    "            ranks.append(0)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "def coverage(top_n_preds, all_possible_items, k=3):\n",
    "    recommended_items = set()\n",
    "    for preds in top_n_preds:\n",
    "        recommended_items.update(preds[:k])\n",
    "    return len(recommended_items) / len(all_possible_items)\n",
    "\n",
    "# Evaluate the recommendations\n",
    "top_3_acc = top_n_accuracy(y_test, top_k_predictions, n=5)\n",
    "mrr = mean_reciprocal_rank(y_test, top_k_predictions)\n",
    "all_possible_items = set(y_train)  # Use the unique items from the training set as all possible items\n",
    "coverage_score = coverage(top_k_predictions, all_possible_items, k=5)\n",
    "\n",
    "print(f\"Top-3 Accuracy: {top_3_acc*100:.1f}%\")\n",
    "print(f\"Mean Reciprocal Rank: {mrr*100:.1f}%\")\n",
    "print(f\"Coverage at 3: {coverage_score*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
