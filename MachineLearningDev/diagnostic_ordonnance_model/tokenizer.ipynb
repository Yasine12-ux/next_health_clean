{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aziz\n",
      "[nltk_data]     Hlila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aziz\n",
      "[nltk_data]     Hlila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLTK's SnowballStemmer and stopwords for French\n",
    "stemmer = SnowballStemmer('french')\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "\n",
    "\n",
    "def tokenizeText(text, dictionary):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text using NLTK tokenizer\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"\\n\\n1️⃣ la tokenisation de texte:\")\n",
    "    print(tokens)\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "    print(\"\\n\\n2️⃣ la suppression des stopwords et les signes de ponctuation:\")\n",
    "    print(tokens)\n",
    "\n",
    "    # Apply stemming using SnowballStemmer\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens ]\n",
    "    print(\"\\n\\n3️⃣ la racinisation:\")\n",
    "    print(stemmed_tokens)\n",
    "\n",
    "    # Numerize tokens\n",
    "    numerical_tokens = []\n",
    "    for token in stemmed_tokens:\n",
    "        if token not in dictionary:\n",
    "            dictionary[token] = len(dictionary) + 1\n",
    "        numerical_tokens.append(dictionary[token])\n",
    "    \n",
    "    # Convert numerical tokens and dictionary values to tensors\n",
    "    numerical_tokens_tensor =numerical_tokens\n",
    "    \n",
    "    print(\"\\n\\n4️⃣ la numérisation de chaque token:\")\n",
    "    print(numerical_tokens_tensor)\n",
    "    print(\"\\n\\n dictionnaire:\")\n",
    "    print(dictionary)\n",
    "    # return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1️⃣ la tokenisation de texte:\n",
      "['le', 'tokenizer', 'nlp', 'segmente', 'le', 'texte', 'en', 'mots', 'et', 'ponctuations', ',', 'facilitant', \"l'analyse\", 'des', 'données', 'textuelles', '.']\n",
      "\n",
      "\n",
      "2️⃣ la suppression des stopwords et les signes de ponctuation:\n",
      "['tokenizer', 'nlp', 'segmente', 'texte', 'mots', 'ponctuations', 'facilitant', 'données', 'textuelles']\n",
      "\n",
      "\n",
      "3️⃣ la racinisation:\n",
      "['tokeniz', 'nlp', 'segment', 'text', 'mot', 'ponctuat', 'facilit', 'don', 'textuel']\n",
      "\n",
      "\n",
      "4️⃣ la numérisation de chaque token:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "\n",
      " dictionnaire:\n",
      "{'tokeniz': 1, 'nlp': 2, 'segment': 3, 'text': 4, 'mot': 5, 'ponctuat': 6, 'facilit': 7, 'don': 8, 'textuel': 9}\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "text = \"Le tokenizer NLP segmente le texte en mots et ponctuations, facilitant l'analyse des données textuelles.\"\n",
    "tokenizeText(text,dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokeniz': 1,\n",
       " 'nlp': 2,\n",
       " 'sépar': 3,\n",
       " 'text': 4,\n",
       " 'mot': 5,\n",
       " 'ponctuat': 6,\n",
       " ',': 7,\n",
       " 'facilit': 8,\n",
       " 'ains': 9,\n",
       " \"l'analys\": 10,\n",
       " 'linguist': 11,\n",
       " 'don': 12,\n",
       " 'textuel': 13,\n",
       " '.': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
